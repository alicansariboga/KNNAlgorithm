# -*- coding: utf-8 -*-
"""dataMiningUsingKNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oop5jbQHczqtLFmjcmDC4D3Durjt-utJ

<b>COMPUTER ENGINEERING

CEN481 - INTRODUCTION TO DATA MINING

ALGORITHM : K-Nearest Neighbour

Ali Can SARIBOÄžA

2019556055</b>
"""

# importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler

# DRIVE CONNECTION

# reading dataset
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dataMining/Acoustic Features.csv')

data.head()

data.info()

# number of the data in each class
song_types = data["Class"].value_counts()
song_types_df = pd.DataFrame(song_types)
song_types_df = song_types.reset_index(level = 0)
song_types_df

song_types_df.groupby('index').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# controlling null value
data.isnull().any().sum()

# distribution percentage of data according to classes
data.Class.value_counts(normalize=True)

# about attributes
data.describe().T

# Correlation
data_lr_corr = data.select_dtypes('number').corr()

plt.figure(figsize=(10,10))
sns.heatmap(data_lr_corr,cmap='RdBu')

plt.figure(figsize=(8,8))
sns.heatmap(data.filter(like='MFCC').corr(),annot=True,fmt = '.2f')

plt.figure(figsize=(8,8))
sns.heatmap(data.filter(like='Chroma').corr(),annot=True)

# importing knn
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedShuffleSplit
from sklearn.metrics import classification_report, accuracy_score,precision_recall_fscore_support, recall_score,f1_score, ConfusionMatrixDisplay, confusion_matrix
import time

X = data.drop('Class', axis = 1)
y = data.loc[:,'Class']

# splitting data
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    shuffle=True,
                                                    random_state = 20,
                                                    stratify=y)

# label encoder
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.fit_transform(y_test)

# standart scaler
mm = StandardScaler()
X_train = mm.fit_transform(X_train)
X_test = mm.transform(X_test)

def get_metric(model,y_test,y_pred):
    """create a series frame showing the accuracy, precision, f1 and recall scores"""
    metric ={}
    metric ['accuracy'] = accuracy_score(y_test,y_pred)
    precision,recall,f_score,support = precision_recall_fscore_support(y_test,y_pred,average='weighted')
    metric['precision'], metric ['recall'] ,metric ['f1']= precision,recall,f_score
    metric ['train score'] = model.score(X_train,y_train)
    return pd.Series(metric)

# selecting best k value for the model
score ={}

for n in range(2,50):
    knn_ = KNeighborsClassifier(n).fit(X_train,y_train)
    score[n] = knn_.score(X_test,y_test)
k_ = max(score, key = score.get)
k_

# KNN Model
start_time = time.time()
knn = KNeighborsClassifier(n_neighbors=k_)
knn.fit(X_train,y_train)
prediction_duration = time.time() - start_time
knn.score(X_test,y_test)

# knn parameters for cross validation
param_grid = {
    'n_neighbors':list(range(2,50)),
    'metric' : ['euclidean','minkowski']
}

from sklearn.model_selection import KFold, StratifiedKFold
# Cross Validation
start_time = time.time()
knn_grid = GridSearchCV(KNeighborsClassifier(),
                        param_grid=param_grid,
                        scoring='accuracy',
                        cv=(StratifiedKFold(n_splits = 10,
                                            shuffle = True,
                                            random_state = 20)))
knn_grid.fit(X_train,y_train)
train_pred = knn_grid.predict(X_train)
test_pred = knn_grid.predict(X_test)
cv_duration = time.time() - start_time

knn_grid.score(X_test,y_test)

knn_grid.best_params_

# Prediction
start_time = time.time()
y_pred_cv = knn_grid.predict(X_test)
prediction_duration = time.time() - start_time

def get_metrics2(y_true, y_pred, duration):
    accuracy = accuracy_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred, average='weighted')
    precision, _, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
    return accuracy, recall, precision, f1, duration

# measurement of train and test
train_accuracy, train_recall, train_precision, train_f1, _ = get_metrics2(y_train, train_pred, 0)
test_accuracy, test_recall, test_precision, test_f1, _ = get_metrics2(y_test, test_pred, prediction_duration)

# comparison between Knn model and Knn cross validation
print("KNN AND KNN_GRID (Cross Validation)\n")
pd.concat([get_metric(knn,y_test,knn.predict(X_test)),
           get_metric(knn_grid,y_test,knn_grid.predict(X_test))],
          axis = 1).rename(columns={0:'knn',1:'knn_grid'})

# performances
print("PERFORMANCE OF TEST AND TRAIN\n")
result_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Recall', 'Precision', 'F1 Score', 'Prediction Duration'],
    'Train': [train_accuracy, train_recall, train_precision, train_f1, 0],
    'Test': [test_accuracy, test_recall, test_precision, test_f1, prediction_duration]
})
print(result_df)

# Results
print("MODEL(knn_grid) RESULTS\n")
print("Best parameters:", knn_grid.best_params_)
print("Average score:", knn_grid.best_score_)
print(f"Training time: {knn_grid.cv_results_['mean_fit_time'][knn_grid.best_index_]:.4f} second")
print(f"Prediction time: {prediction_duration:.4f} second")
print(f"Cross-validation time: {knn_grid.cv_results_['mean_score_time'][knn_grid.best_index_]:.4f} second")

conf = confusion_matrix(y_test,y_pred_cv)
print("Confusion Matrix:")
print(conf)

# Creating heatmap with test data
ax = sns.heatmap(conf,
                 xticklabels=le.classes_,
                 yticklabels = le.classes_,
                 annot= True,
                 cmap = 'winter')

# variation of train and test accuracy according to k value
uzunluk = range(1,50)
error1= []
error2= []
for k in uzunluk:
    classifier= KNeighborsClassifier(n_neighbors=k)
    classifier.fit(X_train,y_train)
    error1.append(classifier.score(X_train, y_train))
    error2.append(classifier.score(X_test, y_test))

plt.figure(figsize=(20,5))
plt.plot(uzunluk,error1,label="train")
plt.plot(uzunluk,error2,label="test")
plt.xlabel('K Value')
plt.ylabel('Accuracy Score')
plt.legend()

# Tags and values
labels = ['Validation Accuracy', 'Model Accuracy']
values = [(knn_grid.score(X_test,y_test)), (knn.score(X_test,y_test))]

# Creating bar chart
plt.figure(figsize=(6, 8))
plt.bar(labels, values, color=['blue', 'green'])

# Show values
for i, value in enumerate(values):
    plt.text(i, value + 0.01, f"{value:.4f}", ha='center', va='bottom', fontsize=12)

# Axes and head
plt.xlabel('Metrics')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy vs Model Accuracy')

plt.show()